<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="StyleSheet" href="style.css" type="text/css" media="all" /> 
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Learning to Poke by Poking</title> 
<style type="text/css">
#primarycontent h1 {
  font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
  text-align: center;
}
#primarycontent p {
  text-align: center;
}
#primarycontent {
  text-align: justify;
}
#primarycontent p {
  text-align: justify;
  padding-left: 10px;
  padding-right: 10px;
}
#primarycontent p iframe {
  text-align: center;
}
.featart {
  margin:4px;
}
.hoverdiv {
  background-color:black;
  margin-top:2px;
  margin-bottom:10px;
  width:100%;
}
.hoverdiv:hover {
  background-color:white;
}
</style>

<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>
<script type="text/javascript"
  src="http://www.maths.nottingham.ac.uk/personal/drw/LaTeXMathML.js">
</script>
<!--
<script type="text/javascript" src="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.js"></script>
<link rel="stylesheet" type="text/css" href="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.standardarticle.css" />
-->
</head> 
<body> 
<div id="primarycontent">
<h1 align="center" itemprop="name"><strong>Learning To Poke by Poking:<br />Experiential Learning of Intuitive Physics</strong></h1>

<center>
<ul id="people" itemprop="accountablePerson">
	<li><h4><a href="http://people.eecs.berkeley.edu/~pulkitag">Pulkit Agrawal*</a>, <a href="https://github.com/anair13">Ashvin Nair*</a>, <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>, 
		<a href="http://people.eecs.berkeley.edu/~malik">Jitendra Malik</a>, <a href="http://homes.cs.washington.edu/~svlevine/">Sergey Levine</a></h4></li>
<li>*Equal Contribution</li>
</ul>
</center>

<img src="img/fig1_cropped.jpg" itemprop="image" width="800" alt="teaserImage">

<h3>Abstract</h3>


<p>We investigate an experiential learning paradigm for acquiring an internal model of
intuitive physics. Our model is evaluated on a real-world robotic manipulation task
that requires displacing objects to target locations by poking. The robot gathered
over 400 hours of experience by executing more than 50K pokes on different
objects. We propose a novel approach based on deep neural networks for modeling
the dynamics of robotâ€™s interactions directly from images, by jointly estimating
forward and inverse models of dynamics. The inverse model objective provides
supervision to construct informative visual features, which the forward model can
then predict and in turn regularize the feature space for the inverse model. The
interplay between these two objectives creates useful, accurate models that can
then be used for multi-step decision making. This formulation has the additional
benefit that it is possible to learn forward models in an abstract feature space and
thus alleviate the need of predicting pixels. Our experiments show that this joint
modeling approach outperforms alternative methods. We also demonstrate that
active data collection using the learned model further improves performance.
</p>

<h3>Data Collection</h3>
<table style="margin: 0 auto">
  <tr>
  <p style="padding-left: 10px; padding-right: 10px;">
   The robot collects the data by randomly poking objects. It is equipped with a Kinect camera, a gripper for poking, and a white rod for resetting the objects to the center of the workspace. The point cloud from the Kinect is used to randomly select one target point for poking. The Kinect point cloud is used only during the training to bias the robot to poke an object instead of poking in free space. The robot pokes objects by moving the finger along the XZ plane at a fixed height from the table. The X and Y axis represent the horizontal and vertical axes, while the Z axis points away from the robot. After selecting a random point to poke ($p$) on the object, the robot randomly samples a poke direction ($\theta$) and length ($l$). Kinematically, the poke is defined by points $p_1, p_2$ that are l/2 distance from $p$ in the directions $\theta$, $(180 + \theta)$ respectively. The robot executes the poke by moving it's finger from $p_1$ to $p_2$. 
  </p>

  <p style="padding-left: 10px; padding-right: 10px;">
  Some sample clips showing the data collection process can be seen in the following video. 
  </p>
    <td>
     <iframe width="640" height="360" src="https://www.youtube.com/embed/4n6fRCvRDlg" frameborder="0" allowfullscreen></iframe> 
    </td>
  </tr>
</table>
<h3>Modelling the effect of pokes is not easy</h3>
  <p style="padding-left: 10px; padding-right: 10px;">
  Modelling the effect of pokes on real world objects is not easy. The complex geometry and material properties of objects often leads to 
  unexpected motion as shown in the following video clip. 
  </p>
<table style="margin: 0 auto">
  <tr>
    <td>
      <iframe width="640" height="360" src="https://www.youtube.com/embed/z2a4uMhRsa4" frameborder="0" allowfullscreen></iframe>
      <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/8SGE1gGi4u8" frameborder="0" allowfullscreen></iframe> -->
    </td>
  </tr>
</table>

<h3>Paper</h3>
  <p style="padding-left: 10px; padding-right: 10px;">
    Preprint of the paper is available at: <a href='http://arxiv.org/abs/1606.07419'>[pdf]</a>
  </p> 

<h3>Supplemental Materials</h3>
  <p style="padding-left: 10px; padding-right: 10px;">
   The robot is tasked with displacing the object from initial to target configuration. Using the multistep decision making proposed in the paper, the final result of the pokes
   is shown as the final state in the image below.  
  </p> 
<img src="img/sequence-no-arrow.png" itemprop="image" alt="result">

  <p style="padding-left: 10px; padding-right: 10px;">
   Additional supplemental materials detailing the simulation experiments will be released soon.
  </p> 

<h3 style="clear:both">Code and Models</h3>
<p style="padding-left: 10px; padding-right: 10px;">
To be released soon. 
</p>

<!-- <h3 style="clear:both">Additional Materials</h3>
-->

<!--
<h3 style="clear:both">Related Papers</h3>
<table>
<tr><td style="border-bottom:solid 1px #ccc">
<p style="padding-left: 10px; padding-right: 10px; margin-top:10px;"><b>Other works on &ldquo;Self-Supervised&rdquo; Learning</b></p></td></tr>
<tr><td>
<p style="padding-left: 10px; padding-right: 10px;">P. Agrawal, J. Carreira and J. Malik <a href="http://www.cs.berkeley.edu/~pulkitag/lsm/lsm.html">Learning to See by Moving</a> ICCV 2015<p>
</td></tr></table>
-->


<h3 style="clear:both">Acknowlegements</h3>
<p style="padding-left: 10px; padding-right: 10px;">
First and foremost, we thank <a href='http://people.eecs.berkeley.edu/~efros/'>Alyosha Efros</a> for inspiration and fruitful discussions throughout this work. The title of this paper has been partly influenced by the term ``pokebot" that Alyosha has been using for several years. We would like to thank Ruzena Bajcsy for access to the Baxter robot and Shubham Tulsiani for helpful comments. This work was supported in part by ONR MURI N00014-14-1-0671 and ONR YIP. Sergey Levine was partially supported by ARL, through the MAST program.  We gratefully acknowledge NVIDIA  corporation for the donation of
K40 GPUs and access to the NVIDIA PSG cluster for this research.

The template for this website has been adopted from Carl Doersch. 
<p style="padding-left: 10px; padding-right: 10px;">
Comments, questions to <a href="http://www.cs.berkeley.edu/~pulkitag"  target="_blank">Pulkit Agrawal</a></p>
</div>

</body>
</html>
